\section{Introduction}\label{sec:intro}

The \textit{conformance testing} of programming language implementations is
crucial to support their consistent execution environments.
%
Many programming languages provide multiple implementations rather than single
reference implementation.
%
For example, Java is an object-oriented language that requires a Java virtual
machine (JVM) to compile programs into JVM bytecode and execute them.
%
Developers could freely choose and install one of the existing JVM
implementations, such as OpenJ9, GraalVM, HotSpot, Zulu, and Corretto.
%
An interpreted programming language, Python, also provides not only the
reference interpreter (CPython) but also diverse other interpreters: PyPy,
Jython, IronPython, etc.
%
Therefore, ensuring consistent execution environments in different
implementations of the same language becomes crucial.
%
Unfortunately, since maintaining a conformance test suite for real-world
programming languages is cumbersome and labor-intensive, only a restricted number
of programming languages, such as JavaScript~\cite{test262} and
XML~\cite{xml-test-suite}, provide their official conformance test suites.
%
Therefore, researchers have presented a way to automatically test the
conformance of multiple implementations using differential
testing~\cite{diff-test} for compilers~\cite{csmith, deep-smith, diff-cpp-front,
diff-test-embedded}, interpreters~\cite{jit-picking, comfort}, virtual
machines~\cite{java-diff-test}, and debuggers~\cite{diff-debugger}.
%
Researchers have utilized various techniques, such as generation-based
fuzzing~\cite{csmith, jit-picking, diff-test-embedded, diff-debugger},
mutation-based fuzzing~\cite{java-diff-test, diff-cpp-front}, and deep
learning~\cite{comfort, deep-smith}, to synthesize diverse programs as inputs of
differential testing for language implementations.

%----------------------------------------%

Measuring the \textit{coverage} of conformance tests is essential from three
points of view: 1) evaluation, 2) maintenance, and 3) generation.
%
First, higher coverage of a conformance test suite denotes that it covers more
test requirements (TRs) of a given coverage criterion for language
implementations.
%
Thus, the measured coverage represents the quality of the test suite.
%
Second, if a tremendous number of conformance tests exist, developers of
language implementations would suffer from the endlessly long time for
conformance testing.
%
To resolve this problem, researchers have presented test minimization
techniques~\cite{test-minimize-survey} to reduce the number of tests, and
\citet{cov-test-minimize} have presented a coverage-guided test minimization
approach.
%
It means we could reduce the number of conformance tests by removing meaningless
conformance tests using coverage information but still sustain the good quality
of conformance testing.
%
Finally, coverage information is also helpful in generating programs for
conformance testing through coverage-guided fuzzing (CGF)~\cite{afl}, a way to
the advance mutational fuzzing by selecting a program as a mutation target using
the coverage information.
%
For example, suppose the current test suite covers only one side of a specific
condition branch.
%
In this case, it is chosen as a target branch to cover its other side, and CGF
selects the program that covers the target branch as a mutation target.

%----------------------------------------%

One way to define the coverage of the conformance test suite is using the graph
coverage in a \textit{mechanized specification} for the language.
%
A possible other approach is to utilize the code coverage, such as structural
coverage or data-flow coverage, in the language implementation as the coverage
of the conformance test suite.
%
However, since a conformance test suite deals with multiple language
implementations, it would have different coverage information for each
implementation.
%
On the other hand, we could define a uniform coverage of the conformance test
suite using the mechanized specification of a programming language.
%
Researchers have presented mechanized specifications to formally describe the
semantics of programming languages, including OCaml~\cite{ocaml-light-spec},
C~\cite{c-light-spec}, C++\cite{cpp-spec}, Java~\cite{k-java},
JavaScript~\cite{jiset}, and POSIX shell~\cite{posix-shell-spec}, using diverse
metalanguage and frameworks, such as Ott~\cite{ott}, Skel~\cite{skel}, and the
$\kframework$ framework~\cite{kframework}.
%
If we could convert a mechanized specification into a directed graph, we could
also define the coverage of a test suite using the graph coverage of the
mechanized specification.
%
For example, $\kjava$~\cite{k-java} is a mechanized specification for Java
defined with the $\kframework$ framework that describes the semantics of the
language as a set of reduction rules.
%
Consider a directed graph whose nodes are reduction rules and edges are
their dependencies in $\kjava$.
%
Then, we can measure the coverage of a test suite based on whether each test
covers the test requirements of a graph coverage criterion in the directed graph
denoting $\kjava$.

%----------------------------------------%

\paragraph{\textbf{Challenges}}
%
Unfortunately, it might degrade the quality of conformance testing in two cases
when sharing the helper functions 1) in the semantics for different language
features or 2) in different parts of the semantics for the same language
features.

%----------------------------------------%

First, mechanized specifications are usually defined in a modular way with
helper functions.
%
Such a modular definition has advantages of preventing redundant or similar
definitions of language semantics, reducing the size of the mechanized
specification, and enhancing its readability.
%
However, it might degrade the quality of conformance testing when sharing the
helper functions in the semantics for different language features.
%
For example, consider a mechanized specification for JavaScript that mimics the
abstract algorithms described in the official language specification,
ECMA-262~\cite{es13}.
%
Here, most of the semantics for the addition and subtraction operators are
defined with a shared \textbf{EvaluateStringOrNumericBinaryExpression} algorithm
as a helper function.
%
If conformance tests for addition operators already cover the possible test
requirements in the algorithm, most conformance tests for subtraction operators
are removed after the coverage-guided test minimization process.
%
However, real-world JavaScript engines are highly optimized and often have
specialized execution paths for different language features.
%
Therefore, we need to test possible edge cases for the subtraction operator as
well even though similar edge cases for the addition operator are already
tested.

%----------------------------------------%

Second, it might also degrade the quality of conformance testing when sharing
the helper functions in different parts of the semantics for the same language
features.
%
For example, consider the mechanized specification for JavaScript again.
%
In JavaScript, the \jscode{String.prototype.normalize} built-in API normalizes a
given string into the normalization form named by a given argument.
%
The definition of the semantics for this built-in API feature utilizes the
\textbf{ToString} algorithm as a helper function twice to represent the
conversion to strings for 1) \jscode{this} value and 2) the first argument of
the API call.
%
Assume that a conformance test suite already covers the test requirements in the
\textbf{ToString} algorithm because of the various values for the \jscode{this}
value.
%
Then, there is no chance to generate new conformance tests that check edge cases
of the conversion from the first argument to string when performing
coverage-guided fuzzing.

%----------------------------------------%

\paragraph{\textbf{This Work}}

To alleviate this problem, we introduce a novel \textit{feature-sensitive (FS)
coverage}, a general extension of graph coverages that discriminates original
test requirements using the most enclosing language features.
%
The FS coverage resolves the problem when sharing the same helper functions
in the semantics for different language features.
%
We also define a \textit{feature call path-sensitive (FCPS) coverage} as its
variant with feature call paths from language features to test requirements.
%
The FCPS coverage resolves the problem when sharing the same helper functions in
different parts of the semantics for the same language features.
%
In addition, we extend them using $k$-limiting approaches as $k$-FS
coverage and $k$-FCPS coverage, respectively.
%
Since JavaScript provides Test262~\cite{test262} as the official conformance
test suite for the language, we select it as the target language to evaluate the
effect of the feature-sensitive coverage and its variants.
%
Then, $\jest$~\cite{jest} is a state-of-the-art JavaScript conformance test
synthesizer using coverage-guided mutational fuzzing, and we implemented $\tool$
by extending it with the family of feature-sensitive coverages
%
For the latest language specification (ES13, 2022), our tool automatically
synthesized \inred{95,000} conformance tests in \inred{50} hours.
%
We checked the conformance of engines and transpilers with the synthesized
conformance tests for evaluation.
%
The evaluation targets were \inred{eight} mainstream tools (\inred{four} engines
and \inred{four} transpilers), and we discovered bugs in all of them.
%
Our tool detected \inred{115} unique conformance bugs (\inred{40} in engines and
\inred{75} in transpilers).
%
We had reported all detected conformance bugs, developers confirmed \inred{80}
bugs, and \inred{40} were newly discovered bugs.

%----------------------------------------%

\paragraph{\textbf{Contributions}}
%
We summarize our contributions as follows:
%
\begin{itemize}

  \item
    We introduce a novel \textit{feature-sensitive (FS) coverage} to discriminate
    test requirements with their enclosing language features to advance
    conformance testing of programming language implementations.
    %
    It could resolve the problem when sharing the same helper
    functions in the semantics for different language features.
  \item 
    We also define \textit{feature call path-sensitive (FCPS) coverage} as its
    variants with feature call paths from language features to test requirements
    to distinguish different parts in the semantics of the same language
    features.

  \item
    We implemented $\tool$ by extending $\jest$, a state-of-the-art conformance
    test synthesizer for JavaScript using coverage-guided mutational fuzzing,
    with the family of feature-sensitive coverage.
    %
    We experimentally show that our tool outperformed the baseline tool in the
    context of conformance bug detection in \inred{eight} mainstream targets
    (\inred{four} engines and \inred{four} transpilers) with the latest ECMA-262
    (ES13, 2022).
\end{itemize}
